<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ICCV 2025 DRL4Real Workshop</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
    <style>
        /* 添加单页应用所需的样式 */
        html {
            scroll-behavior: smooth;
        }
        section {
            padding: 80px 0;
        }
        .section-divider {
            margin: 40px 0;
            border-top: 1px solid #eee;
        }
        /* 邀请嘉宾部分样式 */
        .speakers-section {
            background-color: white;
            padding: 2rem;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
            margin-bottom: 2rem;
        }
        .speakers-section h2 {
            color: #0056b3;
            margin-bottom: 1rem;
        }
        .speakers-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2rem;
            margin-top: 2rem;
        }
        .speaker-card {
            background-color: #f8f9fa;
            padding: 1.5rem;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.05);
            text-align: center;
        }
        .speaker-card img {
            width: 150px;
            height: 150px;
            border-radius: 50%;
            object-fit: cover;
            margin-bottom: 1rem;
        }
        .speaker-card h3 {
            margin-bottom: 0.5rem;
        }
        .speaker-card p {
            color: #666;
            font-size: 0.9rem;
        }
    </style>
</head>
<body>
    <header>
        <div class="logo">
            <h1>ICCV 2025 DRL4Real Workshop</h1>
        </div>
        <nav>
            <ul>
                <li><a href="#home">Home</a></li>
                <li><a href="#introduction">Overview</a></li>
                <li><a href="#dataset">Dataset</a></li>
                <li><a href="#challenge">Challenge</a></li>
                <li><a href="#call-for-papers">Call for Papers</a></li>
                <li><a href="#speakers">Invited Speakers</a></li>
                <!-- <li><a href="#workshop">Workshop Schedule</a></li> -->
                <li><a href="#people">People</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <!-- 首页部分 -->
        <section id="home" class="hero">
            <div class="hero-content">
                <h1>The 1st International Workshop and Challenge on Disentangled Representation Learning for Controllable Generation</h1>
                <h2>ICCV 2025 DRL4Real Workshop</h2>
            </div>
        </section>

        <!-- 介绍部分 -->
        <section id="introduction" class="intro-section">
            <h2 style="font-size: 1.5rem;">01 Overview</h2>
            <h3>Disentangled Representation Learning for Controllable Generation (DRL4Real)</h3>
            <p>Disentangled Representation Learning shows promise for enhancing AI's fundamental understanding of the world, potentially addressing hallucination issues in language models and improving controllability in generative systems. Despite significant academic interest, DRL research remains confined to synthetic scenarios due to a lack of realistic benchmarks and unified evaluation metrics.</p>
            <p>ICCV 2025 DRL4Real Workshop aims to bridge this gap by introducing novel, realistic datasets and comprehensive benchmarks for evaluating DRL methods in practical applications. We will focus on key areas including controllable generation and autonomous driving, exploring how DRL can advance model robustness, interpretability, and generalization capabilities.</p>
        </section>

        <!-- 数据集部分 -->
        <section id="dataset" class="dataset-section">
            <h2>02 Dataset (used for the competition)</h2>

            
            <div class="dataset-description">
                <p>The DRL4Real dataset is specifically designed for disentangled representation learning and controllable generation, covering multiple realistic scenarios and diverse disentanglement factors. It consists of two main categories aligned with our competition tracks:</p>
                <div class="dataset-overview-image">
                    <img src="images/dataset-overview2.png" alt="Dataset Overview" style="width: 120%; max-width: 1071px; height: auto; margin: 20px 0; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                </div>
                <ul>
                    <h3><li><strong>General Image Subset</strong></h3> It Contains approximately 14,000 images across 13 distinct categories: car, cat, cattle, dog, eagle, facial, flower, fruit, horse, pigeon, tree, vegetable, plus time-lapse photography. Images in this subset are partially synthesized using text-to-image generative models, while others are derived from controllable generation video sources or captured from real videos.

                        <ul>
                            <li><strong><em>Single-factor variations</em></strong>: Our dataset provides both text descriptions and corresponding images showing factor changes.</li>
                            <li><strong><em>Multi-factor variations</em></strong>: We primarily provide corresponding text descriptions with a limited number of images as examples.</li>
                        </ul>

                        <!-- 单因子挑战视频展示 -->
                        <div class="single-factor-videos">
                            <h4>Single-Factor Manipulation Examples</h4>
                            <p>Here are demonstration videos showing examples of single-factor manipulation:</p>
                            <div class="video-container">
                                <div class="video-item">
                                    <h5>Car Color Manipulation</h5>
                                    <video controls width="100%">
                                        <source src="video/car_color.mp4" type="video/mp4">
                                        Your browser does not support the video tag.
                                    </video>
                                </div>
                                <div class="video-item">
                                    <h5>Hair Color Manipulation</h5>
                                    <video controls width="100%">
                                        <source src="video/hair color.mp4" type="video/mp4">
                                        Your browser does not support the video tag.
                                    </video>
                                </div>
                                <div class="video-item">
                                    <h5>Rose Color Manipulation</h5>
                                    <video controls width="100%">
                                        <source src="video/rose_color.mp4" type="video/mp4">
                                        Your browser does not support the video tag.
                                    </video>
                                </div>
                            </div>
                        </div>

                        <!-- 多因子挑战图片展示 -->
                        <div class="single-factor-videos">
                            <h4>Multi-Factor Manipulation Examples</h4>
                            <p>Here are demonstration images showing an example of multi-factor manipulation (vase appearance and hand position):</p>
                            <div class="image-container">
                                <div class="image-item">
                                    <h5>Flowers in Vase</h5>
                                    <img src="images/multi_fisrt.png" alt="Flowers in Vase">
                                </div>
                                <div class="image-item">
                                    <h5>Flowers in Hand</h5>
                                    <img src="images/multi_second.png" alt="Flowers in Hand">
                                </div>
                                <div class="image-item">
                                    <h5>Vase in Hand</h5>
                                    <img src="images/multi_third.png" alt="Vase in Hand">
                                </div>
                            </div>
                        </div>
                    </li>
                    
                    <h3><li><strong>Autonomous Driving Subset</strong></h3> A vehicle-centered dataset featuring 5 camera perspectives:
                        <ul>
                             <li>4 outward-facing cameras positioned around the vehicle</li>
                             <li>1 top-down overview camera</li>
                         </ul>
                        <p>Each sample consists of 100 frames extracted from videos captured by these 5 cameras, with simultaneous variations in 8 key factors: vehicle speed (accelerate, constant, decelerate), weather conditions (clear, fog, light rain), lighting, and other environmental factors. The dataset includes 3 distinct vehicle types and more than 5 background scenarios, systematically generated using the CARLA simulator, totaling 20,000 high-quality images.</p>
                        
                        <!-- 自动驾驶数据集视频展示 -->
                        <div class="single-factor-videos">
                            <h4>Autonomous Driving Example</h4>
                            <p>Here is a demonstration video showing an example from the autonomous driving dataset:</p>
                            <div class="video-container">
                                <div class="video-item">
                                    <h5>Autonomous Driving Scene</h5>
                                    <video controls width="100%">
                                        <source src="video/auto.mp4" type="video/mp4">
                                        Your browser does not support the video tag.
                                    </video>
                                </div>
                            </div>
                        </div>

                    </li>
                </ul>
                
                
            </div>
            
            <div class="dataset-download">
                <h3>Dataset Download</h3>
                <p>Access our comprehensive dataset for disentangled representation learning and controllable generation:</p>
                <a href="dataset_download.html" class="download-btn">Download DRL4Real Dataset <i class="fas fa-download"></i></a>
            </div>
            
            <div class="dataset-examples">
                <!-- 暂时隐藏数据集卡片 - Multi-attribute Image 和 Temporal Data 相关内容 -->
                <!--
                <div class="dataset-example">
                    <img src="https://via.placeholder.com/400x300?text=Multi-attribute+Image" alt="Multi-attribute Image">
                    <p><strong>Multi-attribute Image Example</strong></p>
                    <p>This dataset contains images with multiple controllable attributes such as shape, color, texture, pose, etc. The goal of disentangled representation learning is to learn to decouple these attributes into independent representations, allowing each attribute to be controlled individually without affecting others.</p>
                </div>
                
                <div class="dataset-example">
                    <img src="https://via.placeholder.com/400x300?text=Temporal+Data" alt="Temporal Data">
                    <p><strong>Temporal Data Example</strong></p>
                    <p>This dataset contains data that changes over time, used to evaluate the ability of disentangled representation learning to capture temporal dynamics and causal relationships. By learning disentangled representations along the temporal dimension, models can better understand and predict the behavior of complex systems.</p>
                </div>
                -->
            </div>
            
            <!-- <div class="dataset-license">
                <p>This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Researchers are free to use these datasets for academic research, but must provide appropriate citations and follow the license terms.</p>
            </div> -->
        </section>

        <!-- 挑战与竞赛部分 -->
        <section id="challenge" class="challenge-section">
            <h2>03 Challenge</h2>
            
            <div class="challenge-overview">
                <p>Our competition is designed to advance the field of disentangled representation learning through realistic benchmarks and comprehensive evaluation metrics. The challenge is divided into two main tracks:</p>
                
                
            </div>
            
            <div class="challenge-tracks">
                <!-- 单因子赛道 -->
                <div class="challenge-track">
                    <h3>Competition Tracks</h3>
                    
                    <div class="track-section">
                        <h4>1. Single-Factor Track</h4>
                        <p>This track provides over 13,000 images covering scenarios mentioned in the dataset and corresponding text descriptions. Participants <strong>must use disentanglement models</strong> for this track, such as <a href="https://openreview.net/pdf?id=Sy2fzU9gl" target="_blank">β-VAE</a>, <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/bac4d92b3f6decfe47eab9a5893dd1f6-Abstract-Conference.html" target="_blank">GEM</a>, <a href="https://arxiv.org/abs/1812.04948" target="_blank">StyleGAN</a>, <a href="https://arxiv.org/abs/2402.09712" target="_blank">EncDiff</a>, <a href="https://arxiv.org/abs/2402.02346" target="_blank">CL-Dis</a>, <a href="https://arxiv.org/abs/2410.05564" target="_blank">STA</a>. </p>
                        
                        <h5>Evaluation Metrics</h5>
                        <ul>
                            <li><strong>Image Quality (50%):</strong> Measured using Fréchet Inception Distance (FID)</li>
                            <li><strong>Latent Variable Disentanglement (50%):</strong> Evaluated using Bidirectional DCI, which extends the traditional DCI metric by also measuring how perturbations in real images affect the corresponding latent variables in the encoder</li>
                        </ul>
                        <p>For more detailed technical information about the metrics, please see <a href="pdf/tmp.pdf" target="_blank">this PDF</a> (It will be released soon)</p>
                    </div>
                    
                    <div class="track-section">
                        <h4>2. Multi-Factor Track</h4>
                        <p>This track is divided into two sub-categories, each with two separate leaderboards:</p>
                        
                        <div class="multi-factor-category">
                            <h5>2.1 General Image Dataset</h5>
                            <ul>
                                <li><strong>Overall Leaderboard:</strong> Open to all methods. Evaluation uses LLM to assess:
                                    <ul>
                                        <li>Evaluate intensity scoring across a set of images</li>
                                        <li>Ratio of intended attribute changes to unintended changes (closer to 1 is better)</li>
                                    </ul>
                                </li>
                                <li><strong>Disentanglement Leaderboard:</strong> Only for disentanglement models, using the same metrics as the Single-Factor Track</li>
                            </ul>
                            
                        </div>
                        
                        <div class="multi-factor-category">
                            <h5>2.2 Autonomous Dataset</h5>
<ul>
                                <li><strong>Overall Leaderboard:</strong> Open to all methods. Evaluation uses Structural Similarity Index Measure (SSIM) to compare reconstructed frames with ground truth</li>
                                <li><strong>Disentanglement Leaderboard:</strong> Only for disentanglement models, using the same metrics as the Single-Factor Track</li>
                            </ul>
                        </div>
                        <p>For more detailed technical information about the metrics, please see <a href="pdf/tmp.pdf" target="_blank">this PDF</a> (It will be released soon)</p>
                    </div>
                </div>
            </div>
            <div class="prize-info">
                <h4>Prizes for Winning Teams:</h4>
                <ul>
                    <li><strong>First Place:</strong> $800 USD (or equivalent prizes)</li>
                    <li><strong>Second Place:</strong> $400 USD (or equivalent prizes)</li>
                    <li><strong>Third Place:</strong> $200 USD (or equivalent prizes)</li>
                </ul>
                <p>Winners will be invited to submit challenge papers to the DRL4Real 2025 Workshop. Accepted papers will be published in the ICCV 2025 Workshops Proceedings.</p>
            </div>
            
            <div class="challenge-dates">
                <h3>Important Dates</h3>
                <ul>
                    <li><strong>May 31, 23:59:59 AOE:</strong> Workshop Announcement and Training & Validation Dataset Release</li>
                    <li><strong>June 23, 23:59:59 AOE:</strong> Final Test Dataset Release</li>
                    <li><strong>June 30, 23:59:59 AOE:</strong> <a href="https://openreview.net/group?id=thecvf.com/ICCV/2025/Workshop/DRL4Real&referrer=%5BHomepage%5D(%2F)#tab-your-consoles" target="_blank">Workshop Paper Submission</a> Deadline</li>
                    <li><strong>June 30, 23:59:59 AOE:</strong> Competition Submission Deadline</li>
                    <li><strong>July 3, 23:59:59 AOE:</strong> Notification of Competition Results</li>
                    <li><strong>July 7, 23:59:59 AOE:</strong> Challenge Paper Deadline</li>
                    <li><strong>July 11, 23:59:59 AOE:</strong> Notification of Paper Acceptance</li>
                    <li><strong>August 15, 23:59:59 AOE:</strong> Camera Ready Paper Deadline</li>
                    <li><strong>October 19:</strong> Workshop and Competition Results Presentation at ICCV 2025</li>
                </ul>
            </div>
            
            <p>For more details about the challenge, please visit the <a href="https://eval.ai/web/challenges/challenge-page/2527/overview">Challenge</a> page.</p>
        </section>

        <!-- Call for Papers部分 -->
        <section id="call-for-papers" class="call-for-papers-section">
            <h2>04 Call for Papers</h2>
            <div class="call-for-papers-content">
                <p>The DRL4Real Workshop aims to bring together researchers and practitioners from academia and industry to discuss and explore the latest trends, challenges, and innovations in disentangled representation learning and controllable generation. We welcome original research contributions addressing, but not limited to, the following topics:</p>
                
                <ul>
                    <li>Disentangled representation learning</li>
                    <li>Controllable image and video generation</li>
                    <li>Scene understanding and generation in autonomous driving</li>
                    <li>Multi-modal disentanglement (image, video, text)</li>
                    <li>Disentanglement in generative models (GANs, VAEs, Diffusion Models)</li>
                    <li>Applications of disentangled representations in real scensarios</li>
                    <li>Few-shot and zero-shot learning with disentangled representations</li>
                    <li>Integration of large language models for controllable generation</li>
                </ul>
                
                <h3>Submission Details</h3>
                <p>Papers will be peer-reviewed and comply with <a href="https://www.overleaf.com/latex/templates/iccv2025-author-kit/nwnvrwcqwcsh" target="_blank">the ICCV 2025 proceedings style, format and length</a>. The camera-ready deadline aligns with the main conference. Accepted papers must be registered and presented to ensure their inclusion in the IEEE Xplore Library. For details, refer to <a href="https://iccv.thecvf.com/Conferences/2025/AuthorGuidelines" target="_blank">the ICCV 2025 Author Guidelines</a>.</p>
            </div>
        </section>

        <!-- 邀请嘉宾部分 -->
        <section id="speakers" class="speakers-section">
            <h2>05 Invited Speakers</h2>
            <div class="speakers-container">
                <h3 style="text-align: center; margin: 40px 0;">Coming Soon</h3>
            </div>
        </section>

        <!-- 研讨会日程部分 -->
        <!-- <section id="workshop" class="workshop-section">
            <h2>06 Workshop Schedule</h2>
            <h3>DRL4Real: The 1st International Workshop and Challenge on Disentangled Representation Learning for Controllable Generation</h3>
            
            <div class="workshop-schedule">
                <div class="workshop-session">
                    <h3>Part 1</h3>
                    <h4>Introduction & Organizer Talks</h4>
                    <table>
                        <tr>
                            <td>09:00 - 09:10</td>
                            <td>Opening Remarks</td>
                            <td>Professor XXX</td>
                        </tr>
                        <tr>
                            <td>09:10 - 09:30</td>
                            <td>DRL4Real Dataset & Challenge Introduction</td>
                            <td>Professor XXX</td>
                        </tr>
                        <tr>
                            <td>09:30 - 09:50</td>
                            <td>Theoretical Foundations of Disentangled Representation Learning</td>
                            <td>Professor XXX</td>
                        </tr>
                        <tr>
                            <td>09:50 - 10:10</td>
                            <td>Applications of Disentangled Representation Learning in Computer Vision</td>
                            <td>Professor XXX & Professor XXX</td>
                        </tr>
                    </table>
                </div>
                
                <div class="workshop-break">
                    <p>10:10 - 10:30 Coffee Break</p>
                </div>
                
                <div class="workshop-session">
                    <h3>Part 2</h3>
                    <h4>Invited Talks</h4>
                    <table>
                        <tr>
                            <td>10:30 - 11:00</td>
                            <td>Invited Talk 1</td>
                            <td>Professor XXX</td>
                        </tr>
                        <tr>
                            <td>11:00 - 11:30</td>
                            <td>Invited Talk 2</td>
                            <td>Professor XXX</td>
                        </tr>
                        <tr>
                            <td>11:30 - 12:00</td>
                            <td>Invited Talk 3</td>
                            <td>Professor XXX</td>
                        </tr>
                    </table>
                </div>
                
                <div class="workshop-break">
                    <p>12:00 - 13:30 Lunch</p>
                </div>
                
                <div class="workshop-session">
                    <h3>Part 3</h3>
                    <h4>Invited Talks & Challenge Winner Presentations</h4>
                    <table>
                        <tr>
                            <td>13:30 - 14:00</td>
                            <td>Invited Talk 4</td>
                            <td>Professor XXX</td>
                        </tr>
                        <tr>
                            <td>14:00 - 14:30</td>
                            <td>Invited Talk 5</td>
                            <td>Professor XXX</td>
                        </tr>
                        <tr>
                            <td>14:30 - 15:00</td>
                            <td>Invited Talk 6</td>
                            <td>Professor XXX</td>
                        </tr>
                        <tr>
                            <td>15:00 - 15:30</td>
                            <td>Challenge Winner Presentations</td>
                            <td>DRL4Real Challenge Winners</td>
                        </tr>
                    </table>
                </div>
                
                <div class="workshop-break">
                    <p>15:30 - 16:00 Coffee Break</p>
                </div>
                
                <div class="workshop-session">
                    <h3>Part 4</h3>
                    <h4>Poster Session & Closing</h4>
                    <table>
                        <tr>
                            <td>16:00 - 17:30</td>
                            <td>Poster Session & Discussion</td>
                            <td>All Participants</td>
                        </tr>
                        <tr>
                            <td>17:30 - 17:45</td>
                            <td>Closing Remarks</td>
                            <td>Professor XXX</td>
                        </tr>
                    </table>
                </div>
            </div>
        </section> -->

        <!-- 人员部分 -->
        <section id="people" class="people-section">
            <h2>06 People</h2>
            
            <div class="people-category">
                <h3>Organizers</h3>
                <div class="people-grid">
                    <div class="person-card">
                        <img src="images/jinxin.jpg" alt="Xin Jin">
                        <h3>Xin Jin</h3>
                        <p>Assistant Professor</p>
                        <p>Eastern Institute of Technology, Ningbo, China</p>
                        <p><a href="mailto:jinxin@eitech.edu.cn">jinxin@eitech.edu.cn</a></p>
                    </div>

                    <div class="person-card">
                        <img src="images/chenqiuyu.jpg" alt="Qiuyu Chen">
                        <h3>Qiuyu Chen</h3>
                        <p>Ph.D. Candidate</p>
                        <p>Eastern Institute of Technology & Shanghai Jiao Tong University</p>
                        <p><a href="mailto:qychen@eitech.edu.cn">qychen@eitech.edu.cn</a></p>
                    </div>
                    
                    <div class="person-card">
                        <img src="images/songyue.png" alt="Yue Song">
                        <h3>Yue Song</h3>
                        <p>Research Associate</p>
                        <p>California Institute of Technology (Caltech), USA</p>
                        <p><a href="mailto:yue.song@unitn.it">yue.song@unitn.it</a></p>
                    </div>

                    <div class="person-card">
                        <img src="images/XihuiLiu-photo.jpeg" alt="Xihui Liu">
                        <h3>Xihui Liu</h3>
                        <p>Assistant Professor</p>
                        <p>The University of Hong Kong, Hong Kong, China</p>
                        <p><a href="mailto:xihuiliu@eee.hku.hk">xihuiliu@eee.hku.hk</a></p>
                    </div>

                    <div class="person-card">
                        <img src="images/yangshuai.png" alt="Shuai Yang">
                        <h3>Shuai Yang</h3>
                        <p>Assistant Professor</p>
                        <p>Peking University, Beijing, China</p>
                        <p><a href="mailto:williamyang@pku.edu.cn">williamyang@pku.edu.cn</a></p>
                    </div>

                    <div class="person-card">
                        <img src="images/yangtao.jpg" alt="Tao Yang">
                        <h3>Tao Yang</h3>
                        <p>Ph.D.</p>
                        <p>Xi'an Jiaotong University, Xi'an, China</p>
                        <p><a href="mailto:yt14212@stu.xjtu.edu.cn">yt14212@stu.xjtu.edu.cn</a></p>
                    </div>
                    
                    <div class="person-card">
                        <img src="images/liziqiang.jpg" alt="Ziqiang Li">
                        <h3>Ziqiang Li</h3>
                        <p>Ph.D. Candidate</p>
                        <p>Eastern Institute of Technology & Shanghai Jiao Tong University</p>
                        <p><a href="mailto:zqli@eitech.edu.cn">zqli@eitech.edu.cn</a></p>
                    </div>
                    
                    <div class="person-card">
                        <img src="images/huangjianguo.jpg" alt="Jianguo Huang">
                        <h3>Jianguo Huang</h3>
                        <p>Ph.D. Candidate</p>
                        <p>Eastern Institute of Technology, Ningbo, China</p>
                        <p><a href="mailto:huangjgtmyz@gmail.com">huangjgtmyz@gmail.com</a></p>
                    </div>
                    
                    <div class="person-card">
                        <img src="images/weiyuntao.jpg" alt="Yuntao Wei">
                        <h3>Yuntao Wei</h3>
                        <p>Ph.D. Candidate</p>
                        <p>Eastern Institute of Technology & The Hong Kong Polytechnic University</p>
                        <p><a href="mailto:ytwei@eitech.edu.cn">ytwei@eitech.edu.cn</a></p>
                    </div>

                    <div class="person-card">
                        <img src="images/xiebaao.jpg" alt="Baao Xie">
                        <h3>Ba'ao Xie</h3>
                        <p>Research Associate</p>
                        <p>Institute of Digital Twin, EIT, Ningbo, China</p>
                        <p><a href="mailto:bxie@idt.eitech.edu.cn">bxie@idt.eitech.edu.cn</a></p>
                    </div>

                    <div class="person-card">
                        <img src="images/nicusebe.png" alt="Nicu Sebe">
                        <h3>Nicu Sebe</h3>
                        <p>Professor</p>
                        <p>University of Trento, Italy</p>
                        <p><a href="mailto:niculae.sebe@unitn.it">niculae.sebe@unitn.it</a></p>
                    </div>

                    <div class="person-card">
                        <img src="images/zengwenjun.jpg" alt="Wenjun (Kevin) Zeng">
                        <h3>Wenjun (Kevin) Zeng</h3>
                        <p>Professor</p>
                        <p>Eastern Institute of Technology, Ningbo, China</p>
                        <p><a href="mailto:kevzeng@eitech.edu.cn">kevzeng@eitech.edu.cn</a></p>
                    </div>

                    
                </div>
            </div>
            
            <div class="people-category">
                <h3>Assistance</h3>
                <div class="people-grid">
                    <div class="person-card">
                        <h3>Zhenyu Hu</h3>
                        <p>Research Engineer</p>
                        <p>Institute of Digital Twin, EIT, Ningbo, China</p>
                        <p><a href="mailto:zhu@idt.eitech.edu.cn">zhu@idt.eitech.edu.cn</a></p>
                    </div>

                    <div class="person-card">
                        <h3>Shengyang Zhao</h3>
                        <p>Research Associate</p>
                        <p>Institute of Digital Twin, EIT, Ningbo, China</p>
                        <p><a href="mailto:szhao@idt.eitech.edu.cn">szhao@idt.eitech.edu.cn</a></p>
                    </div>

                    <div class="person-card">
                        <h3>Xin Li</h3>
                        <p>Postdoc Research Fellow</p>
                        <p>University of Science and Technology of China, Hefei, China</p>
                        <p><a href="mailto:xin.li@ustc.edu.cn">xin.li@ustc.edu.cn</a></p>
                    </div>
                    
                    <div class="person-card">
                        <h3>Jinming Liu</h3>
                        <p>Ph.D. Candidate</p>
                        <p>Eastern Institute of Technology & Shanghai Jiao Tong University</p>
                        <p><a href="mailto:jmliu206@sjtu.edu.cn">jmliu206@sjtu.edu.cn</a></p>
                    </div>


                    <div class="person-card">
                        <h3>Qi Wang</h3>
                        <p>Ph.D. Candidate</p>
                        <p>Eastern Institute of Technology & Shanghai Jiao Tong University</p>
                        <p><a href="mailto:qiwang067@sjtu.edu.cn">qiwang067@sjtu.edu.cn</a></p>
                    </div>
                    
                    <div class="person-card">
                        <h3>Yunnan Wang</h3>
                        <p>Ph.D. Candidate</p>
                        <p>Eastern Institute of Technology & Shanghai Jiao Tong University</p>
                        <p><a href="mailto:wangyunnan@sjtu.edu.cn">wangyunnan@sjtu.edu.cn</a></p>
                    </div>
                    
                    <div class="person-card">
                        <h3>Wenyao Zhang</h3>
                        <p>Ph.D. Candidate</p>
                        <p>Eastern Institute of Technology & Shanghai Jiao Tong University</p>
                        <p><a href="mailto:wy_zhang@sjtu.edu.cn">wy_zhang@sjtu.edu.cn</a></p>
                    </div>
                    
                    <div class="person-card">
                        <h3>Yunzhong Zheng</h3>
                        <p>Undergraduate</p>
                        <p>New York University, New York, USA</p>
                        <p><a href="mailto:yz9712@nyu.edu">yz9712@nyu.edu</a></p>
                    </div>

                </div>
            </div>
        </section>

        <!-- 联系方式部分 -->
        <section class="contact">
            <h2>Contact</h2>
            <div class="contact-info">
                <div class="contact-person">
                    <h3>Xin Jin</h3>
                    <p>jinxin@eitech.edu.cn</p>
                    <p>Assistant Professor, Eastern Institute of Technology, Ningbo, China</p>
                </div>
                <div class="contact-person">
                    <h3>Qiuyu Chen</h3>
                    <p>canghaimeng@sjtu.edu.cn</p>
                    <p>Ph.D. Candidate, Eastern Institute of Technology, Ningbo, China</p>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <p>&copy; 2025 DRL4Real Workshop. All rights reserved.</p>
    </footer>

    <script src="js/script.js"></script>
    <script>
        // 平滑滚动到锚点
        document.querySelectorAll('nav a').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                
                const targetId = this.getAttribute('href');
                const targetElement = document.querySelector(targetId);
                
                if (targetElement) {
                    window.scrollTo({
                        top: targetElement.offsetTop - 80,
                        behavior: 'smooth'
                    });
                }
            });
        });

        // 高亮当前导航项
        window.addEventListener('scroll', function() {
            const sections = document.querySelectorAll('main > section');
            const navItems = document.querySelectorAll('nav ul li a');
            
            let current = '';
            
            sections.forEach(section => {
                const sectionTop = section.offsetTop - 100;
                const sectionHeight = section.clientHeight;
                if (pageYOffset >= sectionTop && pageYOffset < sectionTop + sectionHeight) {
                    current = section.getAttribute('id');
                }
            });
            
            navItems.forEach(item => {
                item.classList.remove('active');
                if (item.getAttribute('href') === `#${current}`) {
                    item.classList.add('active');
                }
            });
        });
    </script>
</body>
</html>
